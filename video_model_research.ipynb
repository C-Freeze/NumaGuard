{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Imports</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms.functional as tr\n",
    "import torchvision.transforms.v2.functional as trv2\n",
    "from torchvision.transforms import RandomRotation\n",
    "from torchvision.transforms.functional import pil_to_tensor\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import random\n",
    "import hickle\n",
    "\n",
    "from torchvision.models import convnext_tiny, ConvNeXt_Tiny_Weights, resnet50, ResNet50_Weights, regnet_x_400mf, RegNet_X_400MF_Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Global Variables</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_dir = \"/home/tyler/Documents/Data/PinData/PinVideosRaw/data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = \"./best_model/best-parameters.pt\"\n",
    "last_epoch_model_path = \"./best_model/last-epoch-correction-parameters.pt\"\n",
    "best_loss_path = \"./best_model/best-model-loss.txt\"\n",
    "os.makedirs(\"./best_model/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_inputs = 5\n",
    "num_of_outputs = 4\n",
    "num_of_frames = 31\n",
    "\n",
    "hidden_size = 512\n",
    "num_of_rnn_layers = 1\n",
    "embedding_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_WIDTH = 96\n",
    "IMAGE_HEIGHT = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\" )\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_LOSS_KEY = \"Training Loss\"\n",
    "TRAIN_ACCURACY_KEY = \"Training Accuracy\"\n",
    "\n",
    "VAL_LOSS_KEY = \"Validation Loss\"\n",
    "VAL_ACCURACY_KEY = \"Validation Accuracy\"\n",
    "\n",
    "TEST_LOSS_KEY = \"Testing Loss\"\n",
    "TEST_ACCURACY_KEY = \"Testing Accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.LongestMaxSize(IMAGE_HEIGHT, always_apply=True),\n",
    "    # A.GaussianBlur(),\n",
    "    # A.ColorJitter(),\n",
    "    # A.GaussNoise(),\n",
    "    A.Normalize(always_apply=True),\n",
    "    A.ToFloat(always_apply=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Aux Functions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_loss(current_loss: float, model):\n",
    "    \n",
    "    try:\n",
    "        file = open(best_loss_path, \"r+\")\n",
    "    except:\n",
    "        file = open(best_loss_path, \"w+\")\n",
    "        file.write(\"1\")\n",
    "        file.close()\n",
    "        get_best_loss(current_loss, model)\n",
    "        return\n",
    "        \n",
    "    line = file.readline()\n",
    "    best_loss = float(line.replace(\"\\n\", \"\"))\n",
    "\n",
    "    got_new_loss = False\n",
    "\n",
    "    if current_loss < best_loss:\n",
    "        file.seek(0)\n",
    "        file.write(str(current_loss))\n",
    "        file.truncate()\n",
    "\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"New best loss!\")\n",
    "        got_new_loss = True\n",
    "\n",
    "    file.close()\n",
    "\n",
    "    return got_new_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corrects_and_size(pred, y):\n",
    "    \n",
    "    pred = torch.argmax(pred, dim=1)\n",
    "    y = torch.argmax(y, dim=1)\n",
    "    corrects = torch.eq(pred,y).int()\n",
    "    correct = corrects.sum().item()\n",
    "    size = corrects.numel()\n",
    "\n",
    "    return correct, size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_line(list: list):\n",
    "    line = [str(x) for x in list]\n",
    "    line = ','.join(line)\n",
    "    line += \"\\n\"\n",
    "    line = line.replace(\"]\", \"\").replace(\"[\", \"\").replace(\" \", \"\")\n",
    "\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(dataset):\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = int(0.1 * len(dataset))\n",
    "    test_size = len(dataset) - (train_size + val_size) \n",
    "    train_dataset, test_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, test_size, val_size])\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_dataloader, test_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_and_save_graph(metrics:dict, ylabel:str,  key_one:str, key_two:str=None):\n",
    "    \n",
    "    metric_one = metrics[key_one]\n",
    "    metric_two = metrics[key_two]\n",
    "\n",
    "    plt.plot([i + 1 for i in range(len(metric_one))] ,metric_one)\n",
    "    plt.plot([i + 1 for i in range(len(metric_two))] ,metric_two)\n",
    "\n",
    "    if key_two is not None:\n",
    "\n",
    "        title = f'{key_one} and {key_two} vs Epoch'\n",
    "\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.title(title)\n",
    "        plt.legend([key_one, key_two])\n",
    "\n",
    "    else:\n",
    "        title = f'{key_one} vs Epoch'\n",
    "\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.title(title)\n",
    "        plt.legend([key_one])\n",
    "    \n",
    "    plt.grid()\n",
    "\n",
    "    os.makedirs(\"./Figures/\", exist_ok=True) \n",
    "    plt.savefig(f\"./Figures/{title}_{int(time.time())}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_metrics(*args):\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    for key in args:\n",
    "        metrics[key] = []\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_flip_img(img):\n",
    "    global h_flip\n",
    "\n",
    "    if h_flip >= 0.5:\n",
    "        cv2.flip(img, 0, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v_flip_img(img):\n",
    "    global v_flip\n",
    "\n",
    "    if v_flip >= 0.5:\n",
    "        cv2.flip(img, 1, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    global degree\n",
    "\n",
    "    h_flip_img(image)\n",
    "    v_flip_img(image)\n",
    "\n",
    "    image = np.transpose(image, [1,2,0])\n",
    "\n",
    "    image = A.rotate(image, degree)\n",
    "    \n",
    "    image = transform(image=image)[\"image\"]\n",
    "\n",
    "    image = np.transpose(image, [2,0,1])\n",
    "\n",
    "    image = np.expand_dims(image, 0)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frames(frames, mask):\n",
    "\n",
    "    for i, frame in enumerate(frames):\n",
    "\n",
    "        if mask[i] == 0:\n",
    "            break\n",
    "        \n",
    "        frames[i] = preprocess_image(frame.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Custom Dataset Init</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self, csv):\n",
    "        df = pd.read_csv(csv, header=0, dtype=str)\n",
    "        self.df = df\n",
    "        self.predictors = df[\"file_name\"].to_numpy()\n",
    "        self.pins = df[\"pin\"].to_numpy()\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= self.__len__():\n",
    "            raise StopIteration\n",
    "        \n",
    "        global h_flip\n",
    "        global v_flip\n",
    "        global degree\n",
    "        \n",
    "        degree = random.randrange(-360,360)\n",
    "        h_flip = random.random()\n",
    "        v_flip = random.random()\n",
    "        \n",
    "        file_name = self.predictors[idx]\n",
    "        predictors = hickle.load(file_name)[\"image\"]\n",
    "        mask = hickle.load(file_name)[\"mask\"]\n",
    "\n",
    "        preprocess_frames(predictors, mask)\n",
    "\n",
    "        predictors = torch.tensor(predictors, device=device)\n",
    "\n",
    "        pin = str(self.pins[idx])\n",
    "\n",
    "        target_pin = [pin[0], pin[1], pin[2], pin[3]]\n",
    "        target_pin = [int(x) for x in target_pin]\n",
    "\n",
    "        target = torch.zeros((4,10), dtype=torch.float, device=device)\n",
    "\n",
    "        target[0][target_pin[0]] = 1\n",
    "        target[1][target_pin[1]] = 1\n",
    "        target[2][target_pin[2]] = 1\n",
    "        target[3][target_pin[3]] = 1\n",
    "\n",
    "        mask = torch.tensor(mask, device=device)\n",
    "\n",
    "        sample = (predictors, target, mask)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataset = CustomDataSet(\"/home/tyler/Documents/Data/PinData/PinVideosRaw/data.csv\")\n",
    "train_dataloader, test_dataloader, val_dataloader = get_dataloader(custom_dataset)\n",
    "# print(custom_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (predictors, target1, mask) in enumerate(custom_dataset):\n",
    "    print(f'i : {i} input_file: {predictors}{predictors.shape}')\n",
    "    print(f'target1: {target1}{target1.shape}')\n",
    "    print(f'mask: {mask}{mask.shape}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Neural Net Classes</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipConnection(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv2d = nn.Conv3d(in_channels, out_channels, 1, padding=0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv2d(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn1 = nn.Sequential(\n",
    "                nn.Conv3d(in_features, out_features, 3, padding=1),\n",
    "                nn.ELU(),\n",
    "                nn.BatchNorm3d(out_features)\n",
    "        )\n",
    "        self.cnn2 = nn.Sequential(\n",
    "                nn.Conv3d(out_features, out_features, 3, padding=1),\n",
    "                nn.ELU(),\n",
    "                nn.BatchNorm3d(out_features),\n",
    "        )\n",
    "\n",
    "        self.skip1 = SkipConnection(in_features, out_features)\n",
    "        self.pool = nn.MaxPool3d(2)\n",
    "    \n",
    "    def forward(self, x1):\n",
    "\n",
    "        x2 = self.cnn1(x1)\n",
    "        x2 = self.cnn2(x2)\n",
    "\n",
    "        x1 = self.skip1(x1)\n",
    "\n",
    "        x2 += x1\n",
    "\n",
    "        x2 = F.elu(x2)\n",
    "        x2 = self.pool(x2)\n",
    "\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = regnet_x_400mf(weights=RegNet_X_400MF_Weights.DEFAULT)\n",
    "\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size, num_of_rnn_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(hidden_size, 16)\n",
    "        self.fc2 = nn.Linear(16, num_of_outputs)\n",
    "\n",
    "        # for param in self.embedding.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "        # self.embedding.eval()\n",
    "\n",
    "        \n",
    "    def forward(self, x1, mask):\n",
    "\n",
    "\n",
    "        encoded_sequence = torch.zeros((x1.shape[0], hidden_size*2), device=device)\n",
    "\n",
    "        for i, batch in enumerate(x1):\n",
    "\n",
    "            hn = torch.zeros(num_of_rnn_layers*2, hidden_size, requires_grad=True, device=device)\n",
    "\n",
    "            for k, frame in enumerate(batch):\n",
    "\n",
    "                if mask[i,k] == 0:\n",
    "                    break\n",
    "\n",
    "                image_vector = self.embedding(frame.unsqueeze(0))\n",
    "\n",
    "                x1, hn = self.gru(image_vector, hn)\n",
    "\n",
    "            encoded_sequence[i] = torch.concat([hn[0], hn[1]])\n",
    "\n",
    "        return encoded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.gru = nn.GRU(hidden_size*2, hidden_size*2, num_of_rnn_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size*2, 16)\n",
    "        self.fc2 = nn.Linear(16, 10)\n",
    "        \n",
    "    def forward(self, x1):\n",
    "\n",
    "        hn = torch.zeros(num_of_rnn_layers, hidden_size*2, requires_grad=True, device=device)\n",
    "\n",
    "        outputs = torch.zeros((x1.shape[0], 4, 10), device=device)\n",
    "\n",
    "        for i, batch in enumerate(x1):\n",
    "\n",
    "            batch = batch.unsqueeze(0)\n",
    "\n",
    "            for k in range(num_of_outputs):\n",
    "                    \n",
    "                batch, hn = self.gru(batch, hn)\n",
    "\n",
    "                x2 = self.fc1(hn[0])\n",
    "                x2 = F.elu(x2)\n",
    "                x2 = self.fc2(x2)\n",
    "\n",
    "                x2 = x2.unsqueeze(0)\n",
    "                \n",
    "                outputs[i,k] = x2.unsqueeze(0)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.encoder = Encoder()\n",
    "\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "        \n",
    "    def forward(self, x1, mask):\n",
    "        \n",
    "        x1 = self.encoder(x1, mask)\n",
    "\n",
    "        x1 = self.decoder(x1)\n",
    "\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Model Init</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet().to(device)\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total Params: {pytorch_total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion1 = nn.CrossEntropyLoss()\n",
    "criterion2 = nn.CrossEntropyLoss()\n",
    "criterion3 = nn.CrossEntropyLoss()\n",
    "criterion4 = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(list(model.parameters()), lr=0.00001)\n",
    "metrics = init_metrics(TRAIN_LOSS_KEY, TRAIN_ACCURACY_KEY, VAL_LOSS_KEY, VAL_ACCURACY_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Train and Test Init</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, optimizer):\n",
    "    model.train()\n",
    "    \n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    loss, correct, size = 0, 0, 0\n",
    "    correct_makes, correct_misses = 0, 0\n",
    "    incorrect_makes, incorrect_misses = 0, 0\n",
    "\n",
    "    loss_history = metrics[TRAIN_LOSS_KEY]\n",
    "    accuracy_history = metrics[TRAIN_ACCURACY_KEY]\n",
    "    \n",
    "    for X, y1, mask  in dataloader:\n",
    "\n",
    "        pred1 = model(X, mask)\n",
    "\n",
    "        loss = criterion1(pred1, y1)\n",
    "\n",
    "        correct1, size1 = get_corrects_and_size(pred1, y1)\n",
    "\n",
    "        correct += (correct1)\n",
    "        size += (size1)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    loss /= num_batches\n",
    "    loss_history.append(loss.item())\n",
    "    \n",
    "    correct /= size\n",
    "    # precision = correct_makes / (correct_makes + incorrect_makes)\n",
    "    # recall = correct_makes / (correct_makes + incorrect_misses)\n",
    "    accuracy = 100*correct\n",
    "    accuracy_history.append(accuracy)\n",
    "    print(f\"Train Accuracy: {(accuracy):>0.3f}%\\t Train Loss: {loss:>12f}\")\n",
    "    # print(f\"Train Loss: {loss:>12f}\")\n",
    "    # print(f\"Train Precision: {precision}\\t Train Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(dataloader):\n",
    "    size = 0\n",
    "\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss, correct = 0, 0\n",
    "    correct_makes, correct_misses = 0, 0\n",
    "    incorrect_makes, incorrect_misses = 0, 0\n",
    "\n",
    "    loss_history = metrics.get(VAL_LOSS_KEY, [])\n",
    "    accuracy_history = metrics.get(VAL_ACCURACY_KEY, [])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y1, mask in dataloader:\n",
    "\n",
    "            pred1 = model(X, mask)\n",
    "\n",
    "            loss = criterion1(pred1, y1)\n",
    "\n",
    "            correct1, size1 = get_corrects_and_size(pred1, y1)\n",
    "\n",
    "            correct += (correct1)\n",
    "            size += (size1)\n",
    "\n",
    "    loss /= num_batches\n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "    correct /= size\n",
    "    accuracy = 100*correct\n",
    "    # precision = correct_makes / (correct_makes + incorrect_makes)\n",
    "    # recall = correct_makes / (correct_makes + incorrect_misses)\n",
    "    # accuracy_history.append(accuracy)\n",
    "    print(f\"Val Accuracy: {(accuracy):>0.3f}%\\t Val Loss: {loss:>12f}\")\n",
    "    # print(f\"Val Loss: {loss:>12f}\")\n",
    "    # print(f\"Val Precision: {precision}\\t Val Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, loss_fn):\n",
    "    size = 0\n",
    "\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    loss, correct = 0, 0\n",
    "\n",
    "    loss_history = metrics.get(TEST_LOSS_KEY, [])\n",
    "    accuracy_history = metrics.get(TEST_ACCURACY_KEY, [])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y1, y2, y3, y4, mask in dataloader:\n",
    "            \n",
    "            pred = encoder(X)\n",
    "\n",
    "            pred = pred.clamp(0, 1)\n",
    "            pred = torch.nan_to_num(pred)\n",
    "            \n",
    "            loss += loss_fn(pred, y)\n",
    "\n",
    "            pred = torch.argmax(pred, dim=1)\n",
    "            y = torch.argmax(y, dim=1)\n",
    "\n",
    "            corrects = torch.eq(pred,y).int()\n",
    "\n",
    "            correct += corrects.sum().item()\n",
    "\n",
    "            size += corrects.numel()\n",
    "\n",
    "    loss /= num_batches\n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "    correct /= size\n",
    "    accuracy = 100*correct\n",
    "    accuracy_history.append(accuracy)\n",
    "    print(f\"Test Accuracy: {(accuracy):>0.3f}%\\tTest Loss: {loss:>12f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(start, t, t_with_best_loss, best_loss):\n",
    "    train_loss_hist = metrics[TRAIN_LOSS_KEY]\n",
    "    val_loss_hist = metrics[VAL_LOSS_KEY]\n",
    "\n",
    "    if t > 1:\n",
    "        train_loss_dif = (train_loss_hist[-2] - train_loss_hist[-1]) * 100\n",
    "        val_loss_dif = (val_loss_hist[-2] - val_loss_hist[-1]) * 100\n",
    "        train_val_loss_dif = (train_loss_hist[-1] - val_loss_hist[-1]) * 100\n",
    "        print()\n",
    "        print(f\"Train Loss Difference: {train_loss_dif:>0.4f}\\t\\tVal Loss Difference: {val_loss_dif:>0.4f}\")\n",
    "        print(f\"Train Val Loss Difference: {train_val_loss_dif:>0.4f}\")\n",
    "        print()\n",
    "\n",
    "    got_new_loss = get_best_loss(val_loss_hist[-1], model)\n",
    "\n",
    "    if got_new_loss:\n",
    "        t_with_best_loss = t\n",
    "        best_loss = val_loss_hist[-1]\n",
    "    \n",
    "    t_since_best_loss = t - t_with_best_loss\n",
    "    \n",
    "    print(f\"Epoch with best loss: {t_with_best_loss}\\t\\tBest Loss: {best_loss:12f}\")\n",
    "    print(f\"Epochs Since Best Loss: {t_since_best_loss}\")\n",
    "\n",
    "    print(f\"Run Time: {round((time.time() - start), 2)}s\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    return t_with_best_loss, best_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training and Validation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0\n",
    "t_with_best_loss = 0\n",
    "best_loss = 0\n",
    "\n",
    "while True:\n",
    "    start = time.time()\n",
    "    t = t + 1\n",
    "\n",
    "    print(f\"Epoch {t}\\n-------------------------------\")\n",
    "\n",
    "    train(train_dataloader, optimizer)\n",
    "    val(val_dataloader)\n",
    "\n",
    "    t_with_best_loss, best_loss = print_metrics(start, t, t_with_best_loss, best_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
