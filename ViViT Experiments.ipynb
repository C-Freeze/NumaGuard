{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "from vivit import ViViT\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.io as io # for reading video files\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == \"cuda\":\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "    torch.backends.cudnn.enabled = True #Enable cuDNN\n",
    "    torch.backends.cudnn.benchmark = True #Enable cuDNN benchmark for the best performance\n",
    "    warnings.filterwarnings(\"ignore\") #I realy don't care that you broke your elbow\n",
    "\n",
    "print(\"Using device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"batch_size\": 1,\n",
    "    \"epochs\": 50,\n",
    "    \"lr\": 0.001,\n",
    "    \"momentum\": 0.9,\n",
    "    \"log_interval\": 10,\n",
    "    # \"pin_memory\": True,\n",
    "    \"patience\": 20,\n",
    "    \"frame_size\": (224, 224),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 831/831 [02:34<00:00,  5.36it/s]\n"
     ]
    }
   ],
   "source": [
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self, frame_size):\n",
    "        self.root_dir = 'data/videos/'\n",
    "        # Get the list of all files in directory that end in .mp4\n",
    "        self.files = [f for f in os.listdir(self.root_dir) if f.endswith('.mp4')]\n",
    "\n",
    "        # Define the transforms\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.Resize(frame_size),\n",
    "        ])\n",
    "\n",
    "        self.data = []\n",
    "\n",
    "        for file in tqdm(self.files):\n",
    "            # Read the video file\n",
    "            video, audio, info = io.read_video(self.root_dir + file, pts_unit='sec', output_format='tchw')\n",
    "            pin = [int(x) for x in file[0:4]] # Get the pin from the filename (first 4 characters)\n",
    "            pin = torch.tensor(pin, dtype=torch.float)\n",
    "            video = torch.tensor(video, dtype=torch.float32)\n",
    "            \n",
    "            #If the video is too short, skip it\n",
    "            if video.shape[0] != 78:\n",
    "                continue\n",
    "\n",
    "            # Apply the transforms\n",
    "            video = self.transforms(video)\n",
    "            # Append the video to the data\n",
    "            self.data.append((video, pin))      \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Return the video and the pin\n",
    "        return self.data[idx][0], self.data[idx][1]\n",
    "    \n",
    "# Create the datasets\n",
    "dataset = CustomDataSet(params['frame_size'])\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=params[\"batch_size\"], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_correct = 0\n",
    "\n",
    "    for videos, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        videos = videos.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        # optimizer.zero_grad()\n",
    "        #This is a bit faster than the above line\n",
    "        for param in model.parameters():\n",
    "            param.grad = None\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(videos)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_correct += (outputs.argmax(1) == labels).type(torch.float).sum().item()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for videos, labels in tqdm(dataloader, desc=\"Validation\", leave=False):\n",
    "            \n",
    "            videos = videos.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            outputs = model(videos)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, train_loader, val_loader, criterion, optimizer, history):\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(history[\"last_epoch\"] + 1, params[\"epochs\"] + 1):\n",
    "        train_loss = train(\n",
    "            model,\n",
    "            train_loader,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "        )\n",
    "        val_loss = validate(model, val_loader, criterion)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            history[\"best_weights\"] = model.state_dict()\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "\n",
    "        if patience >= params[\"patience\"]:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"last_epoch\"] = epoch\n",
    "\n",
    "        print(f'Epoch:              {epoch}/{params[\"epochs\"]}')\n",
    "        print(f\"Training Loss:      {train_loss:.4f}\")\n",
    "        print(f\"Validation Loss:    {val_loss:.4f}\")\n",
    "        print(f'Learning rate:      {optimizer.param_groups[0][\"lr\"]:.7f}')\n",
    "\n",
    "    return (model, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:              1/50\n",
      "Training Loss:      9.1917\n",
      "Validation Loss:    8.6208\n",
      "Learning rate:      0.0010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:              2/50\n",
      "Training Loss:      8.8329\n",
      "Validation Loss:    7.7743\n",
      "Learning rate:      0.0010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:              3/50\n",
      "Training Loss:      8.8111\n",
      "Validation Loss:    7.8824\n",
      "Learning rate:      0.0010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:              4/50\n",
      "Training Loss:      8.7326\n",
      "Validation Loss:    8.0737\n",
      "Learning rate:      0.0010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:              5/50\n",
      "Training Loss:      8.7094\n",
      "Validation Loss:    7.5900\n",
      "Learning rate:      0.0010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:              6/50\n",
      "Training Loss:      8.7135\n",
      "Validation Loss:    7.5958\n",
      "Learning rate:      0.0010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:              7/50\n",
      "Training Loss:      8.7403\n",
      "Validation Loss:    7.6280\n",
      "Learning rate:      0.0010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:              8/50\n",
      "Training Loss:      8.7432\n",
      "Validation Loss:    7.6116\n",
      "Learning rate:      0.0010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:              9/50\n",
      "Training Loss:      8.6746\n",
      "Validation Loss:    7.7190\n",
      "Learning rate:      0.0010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:              10/50\n",
      "Training Loss:      8.6735\n",
      "Validation Loss:    7.5308\n",
      "Learning rate:      0.0010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:              11/50\n",
      "Training Loss:      8.7136\n",
      "Validation Loss:    7.6322\n",
      "Learning rate:      0.0010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:              12/50\n",
      "Training Loss:      8.6820\n",
      "Validation Loss:    7.5215\n",
      "Learning rate:      0.0010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:              13/50\n",
      "Training Loss:      8.6801\n",
      "Validation Loss:    7.5124\n",
      "Learning rate:      0.0010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:              14/50\n",
      "Training Loss:      8.7050\n",
      "Validation Loss:    7.5374\n",
      "Learning rate:      0.0010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:              15/50\n",
      "Training Loss:      8.6465\n",
      "Validation Loss:    7.5286\n",
      "Learning rate:      0.0010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:              16/50\n",
      "Training Loss:      8.6972\n",
      "Validation Loss:    7.5923\n",
      "Learning rate:      0.0010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:              17/50\n",
      "Training Loss:      8.6287\n",
      "Validation Loss:    7.9100\n",
      "Learning rate:      0.0010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:              18/50\n",
      "Training Loss:      8.6676\n",
      "Validation Loss:    7.5218\n",
      "Learning rate:      0.0010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:              19/50\n",
      "Training Loss:      8.6812\n",
      "Validation Loss:    7.5893\n",
      "Learning rate:      0.0010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training interrupted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "model = ViViT(image_size=params[\"frame_size\"][0], patch_size=16, num_classes=4, num_frames=78).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "\n",
    "patience = 0\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "besst_weights = None\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "try:\n",
    "    model, history = run_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        test_loader,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        history={\n",
    "            \"train_loss\": [],\n",
    "            \"val_loss\": [],\n",
    "            \"val_accuracy\": [],\n",
    "            \"last_epoch\": 0,\n",
    "            \"best_weights\": None,\n",
    "        },\n",
    "    )\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
