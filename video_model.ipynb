{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Imports</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms.functional as tr\n",
    "import torchvision.transforms.v2.functional as trv2\n",
    "from torchvision.transforms import RandomRotation\n",
    "from torchvision.transforms.functional import pil_to_tensor\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Global Variables</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_dir = \"/home/tyler/Downloads/NumaGuard-main/data/videos/clean_2.csv\"\n",
    "src_dir = \"/home/tyler/Downloads/NumaGuard-main/data/videos_2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = \"./best_model/best-parameters.pt\"\n",
    "last_epoch_model_path = \"./best_model/last-epoch-correction-parameters.pt\"\n",
    "best_loss_path = \"./best_model/best-model-loss.txt\"\n",
    "os.makedirs(\"./best_model/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_inputs = 5\n",
    "num_of_outputs = 4\n",
    "num_of_frames = 30\n",
    "\n",
    "hidden_size = 256\n",
    "num_of_rnn_layers = 1\n",
    "embedding_size = 3072"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\" )\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_LOSS_KEY = \"Training Loss\"\n",
    "TRAIN_ACCURACY_KEY = \"Training Accuracy\"\n",
    "\n",
    "VAL_LOSS_KEY = \"Validation Loss\"\n",
    "VAL_ACCURACY_KEY = \"Validation Accuracy\"\n",
    "\n",
    "TEST_LOSS_KEY = \"Testing Loss\"\n",
    "TEST_ACCURACY_KEY = \"Testing Accuracy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Aux Functions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_loss(current_loss: float, model):\n",
    "    \n",
    "    try:\n",
    "        file = open(best_loss_path, \"r+\")\n",
    "    except:\n",
    "        file = open(best_loss_path, \"w+\")\n",
    "        file.write(\"1\")\n",
    "        file.close()\n",
    "        get_best_loss(current_loss, model)\n",
    "        return\n",
    "        \n",
    "    line = file.readline()\n",
    "    best_loss = float(line.replace(\"\\n\", \"\"))\n",
    "\n",
    "    got_new_loss = False\n",
    "\n",
    "    if current_loss < best_loss:\n",
    "        file.seek(0)\n",
    "        file.write(str(current_loss))\n",
    "        file.truncate()\n",
    "\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"New best loss!\")\n",
    "        got_new_loss = True\n",
    "\n",
    "    file.close()\n",
    "\n",
    "    return got_new_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_line(list: list):\n",
    "    line = [str(x) for x in list]\n",
    "    line = ','.join(line)\n",
    "    line += \"\\n\"\n",
    "    line = line.replace(\"]\", \"\").replace(\"[\", \"\").replace(\" \", \"\")\n",
    "\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(dataset):\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = int(0.1 * len(dataset))\n",
    "    test_size = len(dataset) - (train_size + val_size) \n",
    "    train_dataset, test_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, test_size, val_size])\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_dataloader, test_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_and_save_graph(metrics:dict, ylabel:str,  key_one:str, key_two:str=None):\n",
    "    \n",
    "    metric_one = metrics[key_one]\n",
    "    metric_two = metrics[key_two]\n",
    "\n",
    "    plt.plot([i + 1 for i in range(len(metric_one))] ,metric_one)\n",
    "    plt.plot([i + 1 for i in range(len(metric_two))] ,metric_two)\n",
    "\n",
    "    if key_two is not None:\n",
    "\n",
    "        title = f'{key_one} and {key_two} vs Epoch'\n",
    "\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.title(title)\n",
    "        plt.legend([key_one, key_two])\n",
    "\n",
    "    else:\n",
    "        title = f'{key_one} vs Epoch'\n",
    "\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.title(title)\n",
    "        plt.legend([key_one])\n",
    "    \n",
    "    plt.grid()\n",
    "\n",
    "    os.makedirs(\"./Figures/\", exist_ok=True) \n",
    "    plt.savefig(f\"./Figures/{title}_{int(time.time())}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_metrics(*args):\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    for key in args:\n",
    "        metrics[key] = []\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Custom Dataset Init</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self):\n",
    "        df = pd.read_csv(input_file_dir, header=0, dtype=str)\n",
    "        self.df = df\n",
    "        self.predictors = df[\"file_name\"].to_numpy()\n",
    "        self.pins = df[\"pin\"].to_numpy()\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= self.__len__():\n",
    "            raise StopIteration\n",
    "        \n",
    "        file_name = self.predictors[idx]\n",
    "        predictors = torch.load(f\"{src_dir}{file_name}\")\n",
    "        masks = torch.load(f\"{src_dir}{file_name}_mask\")\n",
    "\n",
    "        pin = str(self.pins[idx])\n",
    "\n",
    "        target = [pin[0], pin[1], pin[2], pin[3]]\n",
    "        target = [int(x) for x in target]\n",
    "\n",
    "        target = torch.tensor(target).float() / 9\n",
    "\n",
    "        sample = (predictors, masks, target)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataset = CustomDataSet()\n",
    "train_dataloader, test_dataloader, val_dataloader = get_dataloader(custom_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (sample, mask, output) in enumerate(custom_dataset):\n",
    "    # print(f'i : {i} input_file: {sample}{sample.shape}')\n",
    "    print(f'output_file: {output}{output.shape}')\n",
    "    # print(f'mask: {mask}{mask.shape}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Neural Net Classes</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipConnection(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv2d = nn.Conv2d(in_channels, out_channels, 1, padding=0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv2d(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv2d(in_features, out_features, 3, padding=1),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        self.cnn2 = nn.Sequential(\n",
    "                nn.Conv2d(out_features, out_features, 3, padding=1),\n",
    "                nn.ELU()\n",
    "        )\n",
    "\n",
    "        self.skip1 = SkipConnection(in_features, out_features)\n",
    "        self.pool = nn.MaxPool2d((2,2))\n",
    "        \n",
    "\n",
    "    \n",
    "    def forward(self, x1):\n",
    "\n",
    "        x2 = self.cnn1(x1)\n",
    "        x2 = self.cnn2(x2)\n",
    "\n",
    "        x1 = self.skip1(x1)\n",
    "\n",
    "        x2 += x1\n",
    "\n",
    "        x2 = F.elu(x2)\n",
    "        x2 = self.pool(x2)\n",
    "\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        base = 32\n",
    "\n",
    "        self.start = nn.Sequential(\n",
    "            nn.Conv2d(3, base, 7, padding=3),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d((2,2))\n",
    "        )\n",
    "        \n",
    "        self.block1 = ConvBlock(base, base*2)\n",
    "        # self.block2 = ConvBlock(base*2, base*3)\n",
    "        # self.block3 = ConvBlock(base*3, base*3)\n",
    "        # self.block4 = ConvBlock(base*3, base*3)\n",
    "        self.last = nn.Conv2d(base*2, 1, 1, padding=0)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "    def forward(self, x1):\n",
    "        \n",
    "        x1 = self.start(x1)\n",
    "        x1 = self.block1(x1) \n",
    "        # x1 = self.block2(x1) \n",
    "        # x1 = self.block3(x1) \n",
    "        # x1 = self.block4(x1) \n",
    "        \n",
    "        x1 = self.last(x1)\n",
    "        x1 = F.elu(x1)\n",
    "        x1 = self.flatten(x1)\n",
    "\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size, num_of_rnn_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size, 128)\n",
    "        self.fc2 = nn.Linear(128, num_of_outputs)\n",
    "        \n",
    "    def forward(self, x1, hn):\n",
    "\n",
    "        x1, hn = self.gru(x1, hn)\n",
    "        hn = hn[0]\n",
    "\n",
    "        x1 = self.fc1(hn)\n",
    "        x1 = F.elu(x1)\n",
    "        x1 = self.fc2(x1)\n",
    "\n",
    "        return x1, hn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "    def forward(self, x1, masks):\n",
    "\n",
    "        holder = torch.zeros((x1.shape[0], num_of_outputs), device=device)\n",
    "\n",
    "        for i, batch in enumerate(x1):\n",
    "            \n",
    "            hn = None\n",
    "            yhat = None\n",
    "\n",
    "            for k, frame in enumerate(batch):\n",
    "\n",
    "                mask = masks[i, k]\n",
    "\n",
    "                if mask == 0:\n",
    "                    break\n",
    "\n",
    "                if k == 0:\n",
    "                    hn = torch.zeros(num_of_rnn_layers, hidden_size, requires_grad=True, device=device)\n",
    "\n",
    "                yhat = self.encoder(frame)\n",
    "                yhat, hn = self.decoder(yhat, hn)\n",
    "                # pred = torch.argmax(yhat, dim=0)\n",
    "                hn = hn.unsqueeze(0)\n",
    "                \n",
    "            holder[i] = yhat\n",
    "                \n",
    "        return holder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Model Init</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet().to(device)\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total Params: {pytorch_total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(list(model.parameters()), lr=0.00001)\n",
    "metrics = init_metrics(TRAIN_LOSS_KEY, TRAIN_ACCURACY_KEY, VAL_LOSS_KEY, VAL_ACCURACY_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Train and Test Init</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    size = 0\n",
    "\n",
    "    loss, correct = 0, 0\n",
    "    correct_makes, correct_misses = 0, 0\n",
    "    incorrect_makes, incorrect_misses = 0, 0\n",
    "\n",
    "    loss_history = metrics[TRAIN_LOSS_KEY]\n",
    "    accuracy_history = metrics[TRAIN_ACCURACY_KEY]\n",
    "    \n",
    "    for X, mask, y in dataloader:\n",
    "        X, mask, y = X.to(device), mask.to(device), y.to(device)\n",
    "\n",
    "        pred = model(X, mask)\n",
    "\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # pred = torch.argmax(pred, dim=1)\n",
    "        # y = torch.argmax(y, dim=1)\n",
    "\n",
    "        # for k in range(len(pred)):\n",
    "        #     if pred[k] == 0 and y[k] == 0:\n",
    "        #         correct_makes += 1\n",
    "        #     elif pred[k] == 1 and y[k] == 1:\n",
    "        #         correct_misses += 1\n",
    "        #     elif pred[k] == 0 and y[k] != 0:\n",
    "        #         incorrect_makes += 1\n",
    "        #     elif pred[k] == 1 and y[k] != 1:\n",
    "        #         incorrect_misses += 1\n",
    "\n",
    "        # corrects = torch.eq(pred,y).int()\n",
    "        \n",
    "        # correct += corrects.sum().item()\n",
    "\n",
    "        # size += corrects.numel()\n",
    "\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "        # optimizer.zero_grad()\n",
    "\n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "    if incorrect_makes == 0:\n",
    "        incorrect_makes = 1\n",
    "    \n",
    "    if incorrect_misses == 0:\n",
    "        incorrect_misses = 1\n",
    "    \n",
    "    # correct /= size\n",
    "    # precision = correct_makes / (correct_makes + incorrect_makes)\n",
    "    # recall = correct_makes / (correct_makes + incorrect_misses)\n",
    "    # accuracy = 100*correct\n",
    "    # accuracy_history.append(accuracy)\n",
    "    # print(f\"Train Accuracy: {(accuracy):>0.3f}%\\t Train Loss: {loss:>12f}\")\n",
    "    print(f\"Train Loss: {loss:>12f}\")\n",
    "    # print(f\"Train Precision: {precision}\\t Train Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(dataloader, loss_fn):\n",
    "    size = 0\n",
    "\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss, correct = 0, 0\n",
    "    correct_makes, correct_misses = 0, 0\n",
    "    incorrect_makes, incorrect_misses = 0, 0\n",
    "\n",
    "    loss_history = metrics.get(VAL_LOSS_KEY, [])\n",
    "    accuracy_history = metrics.get(VAL_ACCURACY_KEY, [])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, mask, y in dataloader:\n",
    "            X, mask, y = X.to(device), mask.to(device), y.to(device)\n",
    "\n",
    "            pred = model(X, mask)\n",
    "\n",
    "            loss += loss_fn(pred, y)\n",
    "\n",
    "            # pred = torch.argmax(pred, dim=1)\n",
    "            # y = torch.argmax(y, dim=1)\n",
    "\n",
    "            # for k in range(len(pred)):\n",
    "            #     if pred[k] == 0 and y[k] == 0:\n",
    "            #         correct_makes += 1\n",
    "            #     elif pred[k] == 1 and y[k] == 1:\n",
    "            #         correct_misses += 1\n",
    "            #     elif pred[k] == 0 and y[k] != 0:\n",
    "            #         incorrect_makes += 1\n",
    "            #     elif pred[k] == 1 and y[k] != 1:\n",
    "            #         incorrect_misses += 1\n",
    "\n",
    "            # corrects = torch.eq(pred,y).int()\n",
    "            \n",
    "            # correct += corrects.sum().item()\n",
    "\n",
    "            # size += corrects.numel()\n",
    "\n",
    "    loss /= num_batches\n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "    if incorrect_makes == 0:\n",
    "        incorrect_makes = 1\n",
    "    \n",
    "    if incorrect_misses == 0:\n",
    "        incorrect_misses = 1\n",
    "\n",
    "    # correct /= size\n",
    "    # accuracy = 100*correct\n",
    "    # precision = correct_makes / (correct_makes + incorrect_makes)\n",
    "    # recall = correct_makes / (correct_makes + incorrect_misses)\n",
    "    # accuracy_history.append(accuracy)\n",
    "    # print(f\"Val Accuracy: {(accuracy):>0.3f}%\\t Val Loss: {loss:>12f}\")\n",
    "    print(f\"Val Loss: {loss:>12f}\")\n",
    "    # print(f\"Val Precision: {precision}\\t Val Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, loss_fn):\n",
    "    size = 0\n",
    "\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    loss, correct = 0, 0\n",
    "\n",
    "    loss_history = metrics.get(TEST_LOSS_KEY, [])\n",
    "    accuracy_history = metrics.get(TEST_ACCURACY_KEY, [])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            pred = encoder(X)\n",
    "\n",
    "            pred = pred.clamp(0, 1)\n",
    "            pred = torch.nan_to_num(pred)\n",
    "            \n",
    "            loss += loss_fn(pred, y)\n",
    "\n",
    "            pred = torch.argmax(pred, dim=1)\n",
    "            y = torch.argmax(y, dim=1)\n",
    "\n",
    "            corrects = torch.eq(pred,y).int()\n",
    "\n",
    "            correct += corrects.sum().item()\n",
    "\n",
    "            size += corrects.numel()\n",
    "\n",
    "    loss /= num_batches\n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "    correct /= size\n",
    "    accuracy = 100*correct\n",
    "    accuracy_history.append(accuracy)\n",
    "    print(f\"Test Accuracy: {(accuracy):>0.3f}%\\tTest Loss: {loss:>12f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(start, t, t_with_best_loss, best_loss):\n",
    "    train_loss_hist = metrics[TRAIN_LOSS_KEY]\n",
    "    val_loss_hist = metrics[VAL_LOSS_KEY]\n",
    "\n",
    "    if t > 1:\n",
    "        train_loss_dif = (train_loss_hist[-2] - train_loss_hist[-1]) * 100\n",
    "        val_loss_dif = (val_loss_hist[-2] - val_loss_hist[-1]) * 100\n",
    "        train_val_loss_dif = (train_loss_hist[-1] - val_loss_hist[-1]) * 100\n",
    "        print()\n",
    "        print(f\"Train Loss Difference: {train_loss_dif:>0.4f}\\t\\tVal Loss Difference: {val_loss_dif:>0.4f}\")\n",
    "        print(f\"Train Val Loss Difference: {train_val_loss_dif:>0.4f}\")\n",
    "        print()\n",
    "\n",
    "    got_new_loss = get_best_loss(val_loss_hist[-1], model)\n",
    "\n",
    "    if got_new_loss:\n",
    "        t_with_best_loss = t\n",
    "        best_loss = val_loss_hist[-1]\n",
    "    \n",
    "    t_since_best_loss = t - t_with_best_loss\n",
    "    \n",
    "    print(f\"Epoch with best loss: {t_with_best_loss}\\t\\tBest Loss: {best_loss:12f}\")\n",
    "    print(f\"Epochs Since Best Loss: {t_since_best_loss}\")\n",
    "\n",
    "    print(f\"Run Time: {round((time.time() - start), 2)}s\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    return t_with_best_loss, best_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training and Validation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0\n",
    "t_with_best_loss = 0\n",
    "best_loss = 0\n",
    "\n",
    "while True:\n",
    "    start = time.time()\n",
    "    t = t + 1\n",
    "\n",
    "    print(f\"Epoch {t}\\n-------------------------------\")\n",
    "\n",
    "    train(train_dataloader, optimizer, loss)\n",
    "    val(val_dataloader, loss)\n",
    "\n",
    "    t_with_best_loss, best_loss = print_metrics(start, t, t_with_best_loss, best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "makes = 0\n",
    "misses = 0\n",
    "for sample, target in train_dataloader:\n",
    "    for batch in target:\n",
    "        if batch[0] == 0:\n",
    "            misses += 1\n",
    "        else:\n",
    "            makes += 1\n",
    "\n",
    "print(f\"Makes: {makes} Misses: {misses}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_and_save_graph(metrics, \"Loss\", TRAIN_LOSS_KEY, VAL_LOSS_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1, y = custom_dataset[0]\n",
    "test1 = test1.unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "    pred = model(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred.cpu().squeeze()\n",
    "y = y.squeeze()\n",
    "for i, val in enumerate(pred):\n",
    "    print(f\"pred: {val} y: {y[i]} dif: {(val - y[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i + 1 for i in range(len(pred))] ,pred)\n",
    "plt.plot([i + 1 for i in range(len(y))] ,y)\n",
    "plt.legend([\"Pred\", \"Y\"])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
